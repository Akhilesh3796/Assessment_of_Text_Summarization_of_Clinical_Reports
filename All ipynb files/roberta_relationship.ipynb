{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'textattack/roberta-base-MNLI' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'textattack/roberta-base-MNLI' was a path or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Load the NLI model and tokenizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtextattack/roberta-base-MNLI\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_relationship\u001b[39m(text1, text2):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Tokenize the input texts\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_auto.py:112\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m CamembertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    111\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m pretrained_model_name_or_path:\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m RobertaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    113\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m pretrained_model_name_or_path:\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils.py:283\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.from_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    239\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m    Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils.py:341\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m         vocab_files[file_id] \u001b[39m=\u001b[39m full_file_name\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 341\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    342\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mModel name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m was not found in tokenizers model name list (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    343\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWe assumed \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m was a path or url to a directory containing vocabulary files \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    344\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnamed \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m but couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find such vocabulary files at this path or url.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    345\u001b[0m                 pretrained_model_name_or_path, \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(s3_models),\n\u001b[0;32m    346\u001b[0m                 pretrained_model_name_or_path, \n\u001b[0;32m    347\u001b[0m                 \u001b[39mlist\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mvocab_files_names\u001b[39m.\u001b[39mvalues())))\n\u001b[0;32m    349\u001b[0m \u001b[39m# Get files from url, cache, or disk depending on the case\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: Model name 'textattack/roberta-base-MNLI' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'textattack/roberta-base-MNLI' was a path or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the NLI model and tokenizer\n",
    "model_name = \"textattack/roberta-base-MNLI\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def classify_relationship(text1, text2):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer.encode_plus(text1, text2, add_special_tokens=True, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    # Pass the input through the NLI model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label = outputs.logits.argmax().item()\n",
    "\n",
    "    # Map the label to the relationship type\n",
    "    relationships = {0: \"entailment\", 1: \"contradiction\"}\n",
    "    relationship_type = relationships.get(predicted_label)\n",
    "\n",
    "    return relationship_type\n",
    "\n",
    "def classify_relationship_csv(csv_file, column1, column2):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Select the specified columns for relationship classification\n",
    "    text_data1 = df[column1].tolist()\n",
    "    text_data2 = df[column2].tolist()\n",
    "\n",
    "    # Classify relationship for each pair of texts\n",
    "    relationships = []\n",
    "    for text1, text2 in zip(text_data1, text_data2):\n",
    "        relationship = classify_relationship(text1, text2)\n",
    "        relationships.append(relationship)\n",
    "\n",
    "    # Add the relationships to the DataFrame\n",
    "    df['relationship'] = relationships\n",
    "\n",
    "    return df\n",
    "\n",
    "# Specify the CSV file path and the columns for relationship classification\n",
    "csv_file = 'D:/Thesis/Processed Data/GPT-test-summary.csv'\n",
    "column1 = 'Statement'\n",
    "column2 = 'summary'\n",
    "\n",
    "# Classify relationship for the CSV file\n",
    "relationship_df = classify_relationship_csv(csv_file, column1, column2)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_csv = 'D:/Thesis/Processed Data/roberta-test-summary-relationship.csv'\n",
    "relationship_df.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.test' has no attribute 'is_built_with_cudnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mtest\u001b[39m.\u001b[39mis_gpu_available(cuda_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, min_cuda_compute_capability\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      5\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGPU is available\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCuDNN is enabled:\u001b[39m\u001b[39m\"\u001b[39m, tf\u001b[39m.\u001b[39;49mtest\u001b[39m.\u001b[39;49mis_built_with_cudnn())\n\u001b[0;32m      7\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGPU is not available\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.test' has no attribute 'is_built_with_cudnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available and if CuDNN is enabled\n",
    "if tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None):\n",
    "  print(\"GPU is available\")\n",
    "  print(\"CuDNN is enabled:\", tf.test.is_built_with_cudnn())\n",
    "else:\n",
    "  print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\akhil\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (1.25.1)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
